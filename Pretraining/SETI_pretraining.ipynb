{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "SETI-Pretraining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfYTUE54WbHK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr3YeP1uGLQl"
      },
      "source": [
        "!pip install fsspec -q\n",
        "!pip install gcsfs -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIzNuEcPXWyE"
      },
      "source": [
        "!pip install tensorflow_addons -q\n",
        "!pip install -U efficientnet -q\n",
        "!pip install vit-keras -q\n",
        "!pip install git+https://github.com/qubvel/classification_models.git -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kle-AJeDfCOE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/TFNFNet')\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import metrics\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# policy = tf.keras.mixed_precision.Policy('mixed_bfloat16')\n",
        "# tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "import efficientnet.tfkeras as efn\n",
        "from vit_keras import vit\n",
        "from classification_models.tfkeras import Classifiers\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "from nfnet import NFNet\n",
        "\n",
        "def set_seed(seed = 0):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMWnH89Ceh-p"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MShlpULIj7ZW"
      },
      "source": [
        "EPOCHS = 5\n",
        "FOLDS = 5\n",
        "TRAIN_FOLD = 1 # 0 -> 1 -> 2 -> 3 -> 4\n",
        "OLD_DIM = 256\n",
        "DIM = 600\n",
        "SEED = 42\n",
        "IMAGE_SIZE = [DIM, DIM]\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "GCS_DS_PATH = 'gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4'\n",
        "LR = 5e-5\n",
        "BATCH_SIZE = strategy.num_replicas_in_sync*4\n",
        "PRETRAINED_MODELV2 =  'efficientnetv2-l-21k'\n",
        "PRETRAINED_MODEL = 'seresnext50'\n",
        "NFNET_VARIANT = 'F3'\n",
        "VIT_VARIANT = 'vit_b32'\n",
        "hub_url = f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/{PRETRAINED_MODELV2}/feature-vector'\n",
        "MODEL_GCS_PATH = 'gs://kds-5b3917d3d77c2d5e35aa3dff601deb72749b3b749ff524b335978b15'\n",
        "\n",
        "MODELS = {\n",
        "    'B0': efn.EfficientNetB0,\n",
        "    'B1': efn.EfficientNetB1,\n",
        "    'B2': efn.EfficientNetB2,\n",
        "    'B3': efn.EfficientNetB3,\n",
        "    'B4': efn.EfficientNetB4,\n",
        "    'B5': efn.EfficientNetB5,\n",
        "    'B6': efn.EfficientNetB6,\n",
        "    'B7': efn.EfficientNetB7\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N73RGefFkQkg"
      },
      "source": [
        "labels = pd.read_csv('/content/drive/MyDrive/Kaggle/train_labels.csv')\n",
        "labels['path'] = labels['id'].progress_apply(lambda x:  f\"{GCS_DS_PATH}/train/{x[0]}/{x}.npy\")\n",
        "old_labels = pd.read_csv('/content/drive/MyDrive/Kaggle/train_labels_old.csv')\n",
        "old_labels['path'] = old_labels['id'].progress_apply(lambda x:  f\"{GCS_DS_PATH}/old_leaky_data/train_old/{x[0]}/{x}.npy\")\n",
        "# labels = labels.append(old_labels).reset_index(drop=True)\n",
        "labels = old_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUuKcsQIkQeF"
      },
      "source": [
        "def build_decoder(with_labels=True, target_size=(256, 256), ext='npy'):\n",
        "    def decode(path):\n",
        "        file_bytes = tf.io.read_file(path)\n",
        "        if ext == 'npy':\n",
        "            img = tf.io.decode_raw(file_bytes, tf.float16)\n",
        "            img = img[64:]\n",
        "            img = tf.reshape(img, [6, 273, 256])\n",
        "            img = tf.concat([img[0,:,:], img[2,:,:], img[4,:,:]], axis=0)\n",
        "            img = tf.stack([img, img, img], axis=-1)\n",
        "            img = tf.cast(img, tf.float32) \n",
        "            img = tf.image.resize(img, target_size)\n",
        "        else:\n",
        "            if ext == 'png':\n",
        "                img = tf.image.decode_png(file_bytes, channels=3)\n",
        "            elif ext in ['jpg', 'jpeg']:\n",
        "                img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
        "            else:\n",
        "                raise ValueError(\"Image extension not supported\")\n",
        "            img = tf.cast(img, tf.float32) / 255.0\n",
        "            img = tf.image.resize(img, target_size)\n",
        "\n",
        "        return img\n",
        "    \n",
        "    def decode_with_labels(path, label):\n",
        "        return decode(path), label\n",
        "    \n",
        "    return decode_with_labels if with_labels else decode\n",
        "\n",
        "\n",
        "def build_augmenter(with_labels=True):\n",
        "    def augment(img):\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_flip_up_down(img)\n",
        "        return img\n",
        "    \n",
        "    def augment_with_labels(img, label):\n",
        "        return augment(img), label\n",
        "    \n",
        "    return augment_with_labels if with_labels else augment\n",
        "\n",
        "\n",
        "def build_dataset(paths, labels=None, bsize=128, cache=True,\n",
        "                  decode_fn=None, augment_fn=None,\n",
        "                  augment=True, repeat=True, shuffle=1024, \n",
        "                  cache_dir=\"\"):\n",
        "    if cache_dir != \"\" and cache is True:\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "    \n",
        "    if decode_fn is None:\n",
        "        decode_fn = build_decoder(labels is not None)\n",
        "    \n",
        "    if augment_fn is None:\n",
        "        augment_fn = build_augmenter(labels is not None)\n",
        "    \n",
        "    AUTO = tf.data.experimental.AUTOTUNE\n",
        "    slices = paths if labels is None else (paths, labels)\n",
        "    \n",
        "    dset = tf.data.Dataset.from_tensor_slices(slices)\n",
        "    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n",
        "    dset = dset.cache(cache_dir) if cache else dset\n",
        "    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n",
        "    dset = dset.repeat() if repeat else dset\n",
        "    dset = dset.shuffle(shuffle) if shuffle else dset\n",
        "    dset = dset.batch(bsize).prefetch(AUTO)\n",
        "    \n",
        "    return dset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43FNnTKPkp_U"
      },
      "source": [
        "def transform(image,label):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rot = 1.* tf.random.normal([1], dtype='float32', seed=42)\n",
        "    shr = tf.random.normal([1], dtype='float32', seed=42) \n",
        "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32', seed=42)/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32', seed=42)/10.\n",
        "    h_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "    w_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "  \n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = tf.keras.backend.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = tf.keras.backend.cast(idx2,dtype='int32')\n",
        "    idx2 = tf.keras.backend.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
        "        \n",
        "    return tf.reshape(d,[DIM,DIM,3]),label\n",
        "\n",
        "def transform_test(image):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rot = 1.* tf.random.normal([1], dtype='float32', seed=42)\n",
        "    shr = tf.random.normal([1], dtype='float32', seed=42) \n",
        "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32', seed=42)/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32', seed=42)/10.\n",
        "    h_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "    w_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "  \n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = tf.keras.backend.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = tf.keras.backend.cast(idx2,dtype='int32')\n",
        "    idx2 = tf.keras.backend.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
        "        \n",
        "    return tf.reshape(d,[DIM,DIM,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8asAWQokp3d"
      },
      "source": [
        "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
        "    # returns 3x3 transformmatrix which transforms indicies\n",
        "        \n",
        "    # CONVERT DEGREES TO RADIANS\n",
        "    rotation = math.pi * rotation / 180.\n",
        "    shear = math.pi * shear / 180.\n",
        "    \n",
        "    # ROTATION MATRIX\n",
        "    c1 = tf.math.cos(rotation)\n",
        "    s1 = tf.math.sin(rotation)\n",
        "    one = tf.constant([1],dtype='float32')\n",
        "    zero = tf.constant([0],dtype='float32')\n",
        "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
        "        \n",
        "    # SHEAR MATRIX\n",
        "    c2 = tf.math.cos(shear)\n",
        "    s2 = tf.math.sin(shear)\n",
        "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
        "    \n",
        "    # ZOOM MATRIX\n",
        "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    # SHIFT MATRIX\n",
        "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
        "    \n",
        "    return tf.keras.backend.dot(tf.keras.backend.dot(rotation_matrix, shear_matrix), tf.keras.backend.dot(zoom_matrix, shift_matrix))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXmVPFeVkwA0"
      },
      "source": [
        "AUG_BATCH = BATCH_SIZE\n",
        "\n",
        "def cutmix(image, label, PROBABILITY = 1.0):\n",
        "    \n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with cutmix applied\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    CLASSES = 1\n",
        "    label = tf.cast(label, 'float32')\n",
        "    \n",
        "    imgs = []; labs = []\n",
        "    for j in range(AUG_BATCH):\n",
        "        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n",
        "        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n",
        "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
        "        # CHOOSE RANDOM LOCATION\n",
        "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
        "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
        "        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n",
        "        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\n",
        "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
        "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
        "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
        "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
        "        # MAKE CUTMIX IMAGE\n",
        "        one = image[j,ya:yb,0:xa,:]\n",
        "        two = image[k,ya:yb,xa:xb,:]\n",
        "        three = image[j,ya:yb,xb:DIM,:]\n",
        "        middle = tf.concat([one,two,three],axis=1)\n",
        "        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n",
        "        imgs.append(img)\n",
        "        # MAKE CUTMIX LABEL\n",
        "        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n",
        "\n",
        "        lab1 = label[j,]\n",
        "        lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "            \n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH, DIM,DIM,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH, 1))\n",
        "    return image2,label2\n",
        "\n",
        "def mixup(image, label, PROBABILITY = 1.0):\n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with mixup applied\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    CLASSES = 1\n",
        "    label = tf.cast(label, 'float32')\n",
        "    \n",
        "    imgs = []; labs = []\n",
        "    for j in range(AUG_BATCH):\n",
        "        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n",
        "        # CHOOSE RANDOM\n",
        "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
        "        a = 0.5 #tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\n",
        "        # MAKE MIXUP IMAGE\n",
        "        img1 = image[j,]\n",
        "        img2 = image[k,]\n",
        "        imgs.append((1-a)*img1 + a*img2)\n",
        "        # MAKE CUTMIX LABEL\n",
        "\n",
        "        lab1 = label[j,]\n",
        "        lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "\n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,1))\n",
        "    return image2,label2\n",
        "\n",
        "def transform_cutmix_mixup(image,label):\n",
        "    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    SWITCH = 0.5\n",
        "    CUTMIX_PROB = 0.666\n",
        "    MIXUP_PROB = 0.666\n",
        "    label = tf.cast(label, 'float32')\n",
        "    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n",
        "    image2, label2 = cutmix(image, label, CUTMIX_PROB)\n",
        "    image3, label3 = mixup(image, label, MIXUP_PROB)\n",
        "    imgs = []; labs = []\n",
        "    for j in range(AUG_BATCH):\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n",
        "        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n",
        "        labs.append(P*label2[j,]+(1-P)*label3[j,])\n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
        "    label4 = tf.reshape(tf.stack(labs),(AUG_BATCH,1))\n",
        "    return image4,tf.reshape(tf.cast(tf.math.round(label4), 'int64'), (-1,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTt5CYu6kv5l"
      },
      "source": [
        "# One of:\n",
        "\n",
        "##################### Qubvel's Effnets #####################\n",
        "\n",
        "# def CNN(MODEL):\n",
        "#     model = tf.keras.models.Sequential()\n",
        "#     model.add(MODEL(weights='imagenet', include_top=False, input_shape=(*(IMAGE_SIZE), 3)))\n",
        "#     model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "#     model.add(tf.keras.layers.Dropout(0.5))\n",
        "#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "#         loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "#         metrics=[tf.keras.metrics.AUC(name='AUC')]\n",
        "#     )\n",
        "    \n",
        "#     return model\n",
        "\n",
        "##################### Qubvel's Classifiers #####################\n",
        "\n",
        "# def CNN():\n",
        "#     model = tf.keras.models.Sequential()\n",
        "#     model.add(Classifiers.get(PRETRAINED_MODEL)[0](weights='imagenet', include_top=False, input_shape=(*(IMAGE_SIZE), 3)))\n",
        "#     model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "#     model.add(tf.keras.layers.Dropout(0.5))\n",
        "#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=tfa.optimizers.SWA(tf.keras.optimizers.Adam(learning_rate=LR)),\n",
        "#         loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "#         metrics=[tf.keras.metrics.AUC(name='AUC')]\n",
        "#     )\n",
        "    \n",
        "#     return model\n",
        "\n",
        "##################### Qubvel's EffnetL2 with xhlulu's pretrained weights #####################\n",
        "\n",
        "# def CNN():\n",
        "#     model = tf.keras.models.Sequential()\n",
        "#     model.add(efn.EfficientNetL2(weights='/content/drive/MyDrive/EfficientNetL2_pretrained/efficientnet-l2_noisy-student_notop.h5', drop_connect_rate=0, include_top=False, input_shape=(*(IMAGE_SIZE), 3)))\n",
        "#     model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "#     model.add(tf.keras.layers.Dropout(0.5))\n",
        "#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=tfa.optimizers.SWA(tf.keras.optimizers.Adam(learning_rate=LR), start_averaging=2),\n",
        "#         loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "#         metrics=[tf.keras.metrics.AUC(name='AUC')]\n",
        "#     )\n",
        "    \n",
        "#     return model\n",
        "\n",
        "##################### faustomorales's ViT #####################\n",
        "\n",
        "# def CNN():\n",
        "#   model = vit.vit_b32(\n",
        "#       image_size=(DIM, DIM),\n",
        "#       activation='sigmoid',\n",
        "#       pretrained=True,\n",
        "#       include_top=True,\n",
        "#       pretrained_top=False,\n",
        "#       classes=1\n",
        "#   )\n",
        "\n",
        "#   model.compile(\n",
        "#       optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "#       loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "#       metrics=[tf.keras.metrics.AUC(name='AUC')]\n",
        "#   )\n",
        "  \n",
        "#   return model\n",
        "\n",
        "##################### Google's EffnetV2 #####################\n",
        "\n",
        "# def CNN():\n",
        "#     model = tf.keras.models.Sequential()\n",
        "#     model.add(tf.keras.layers.InputLayer(input_shape=(DIM, DIM, 3)))\n",
        "#     model.add(hub.KerasLayer(hub_url, trainable=True))\n",
        "#     model.add(tf.keras.layers.Dropout(0.5))\n",
        "#     model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "#         loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "#         metrics=[tf.keras.metrics.AUC(name='AUC')]\n",
        "#     )\n",
        "    \n",
        "#     return model\n",
        "\n",
        "##################### Google's NFNet with hoangthang1607's pretrained weights #####################\n",
        "\n",
        "def CNN():\n",
        "    \n",
        "    inputs = tf.keras.layers.Input(shape=(DIM, DIM, 3))\n",
        "    \n",
        "    nfnet_model = NFNet(\n",
        "        num_classes=1000, \n",
        "        variant=NFNET_VARIANT,\n",
        "    )    \n",
        "    nfnet_model.load_weights(f'{MODEL_GCS_PATH}/NFNets_weights/{NFNET_VARIANT}_NFNet/{NFNET_VARIANT}_NFNet')\n",
        "    \n",
        "    x = nfnet_model(inputs)['pool']\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    model.build((None, DIM, DIM, 3))\n",
        "       \n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.AUC(name='AUC')]\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O51CGbYij7O2"
      },
      "source": [
        "LR_START = LR\n",
        "LR_MAX = LR*4\n",
        "LR_MIN = LR/10\n",
        "LR_RAMPUP_EPOCHS = 5\n",
        "LR_SUSTAIN_EPOCHS = 0\n",
        "LR_EXP_DECAY = .8\n",
        "\n",
        "def CustomSchedule(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
        "    return lr\n",
        "\n",
        "def ExponentialSchedule(epoch, lr):\n",
        "    rate = DECAY_RATE\n",
        "    return lr*tf.math.exp(-rate*(epoch)).numpy()\n",
        "\n",
        "\n",
        "CustomCallback = tf.keras.callbacks.LearningRateScheduler(CustomSchedule, verbose = True)\n",
        "EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_AUC', patience=10, mode='max', restore_best_weights=True)\n",
        "ReduceOnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_AUC', mode='max', min_lr=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dma81Audk5i2"
      },
      "source": [
        "paths = labels['path'].values\n",
        "targets = labels['target'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTO4DP3qk5YM"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
        "oof_preds = pd.DataFrame({'True': targets, 'Preds': np.zeros(len(targets))})\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(skf.split(paths, targets)):\n",
        "  if fold == TRAIN_FOLD:\n",
        "      print('Fold:', fold)\n",
        "\n",
        "      X_filenames, Xvalid_filenames = paths[train_idx], paths[valid_idx]\n",
        "      train_targets, valid_targets = targets[train_idx], targets[valid_idx]\n",
        "\n",
        "      TRAIN_STEPS = X_filenames.shape[0]//BATCH_SIZE\n",
        "      VALID_STEPS = Xvalid_filenames.shape[0]//BATCH_SIZE\n",
        "\n",
        "      decoder = build_decoder(with_labels=True, target_size=IMAGE_SIZE, ext='npy')\n",
        "      train_dataset = build_dataset(\n",
        "          X_filenames, \n",
        "          train_targets,\n",
        "          bsize=BATCH_SIZE, \n",
        "          decode_fn=decoder\n",
        "      )\n",
        "      valid_dataset = build_dataset(\n",
        "          Xvalid_filenames,\n",
        "          valid_targets,\n",
        "          bsize=BATCH_SIZE, \n",
        "          decode_fn=decoder,\n",
        "          shuffle=False, \n",
        "          repeat=False, \n",
        "          augment=False\n",
        "      )\n",
        "      train_dataset = train_dataset.map(mixup, num_parallel_calls=AUTO)\n",
        "\n",
        "      with strategy.scope():\n",
        "          model = CNN()\n",
        "\n",
        "      model.fit(\n",
        "          train_dataset,\n",
        "          epochs=EPOCHS,\n",
        "          steps_per_epoch=TRAIN_STEPS,\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=VALID_STEPS,\n",
        "          callbacks=[ReduceOnPlateau, EarlyStopping]\n",
        "      )\n",
        "\n",
        "      y_pred = model.predict(valid_dataset).reshape(-1,)\n",
        "      print('AUC:', metrics.roc_auc_score(valid_targets, y_pred))\n",
        "      \n",
        "      # model.save(f'/content/drive/MyDrive/Kaggle/weights/Pretrained_weights/EfficientNet-L2/efficientnet_l2_mixup_{fold}_SWA.h5')\n",
        "      # model.save(f'/content/drive/MyDrive/Kaggle/weights/Pretrained_weights/VIT-B32/{VIT_VARIANT}_{fold}.h5')\n",
        "\n",
        "      save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "      model.save(f'/content/drive/MyDrive/Kaggle/weights/Pretrained_weights/NFNet-{NFNET_VARIANT}/{fold}', options=save_locally)\n",
        "\n",
        "      oof_preds.loc[valid_idx, 'Preds'] = y_pred\n",
        "\n",
        "      del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7avPgcqkZsOC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}