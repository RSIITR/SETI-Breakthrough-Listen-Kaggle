{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SETI_Inference - 5x TTA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI5KzneMdZIh"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTScChTSdgTz"
      },
      "source": [
        "!pip install git+https://github.com/qubvel/classification_models.git -q\n",
        "!pip install -U efficientnet -q\n",
        "!pip install tensorflow_addons -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVxnYC49dgRe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/TFNFNet')\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn import metrics\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import efficientnet.tfkeras as efn\n",
        "from classification_models.tfkeras import Classifiers\n",
        "from nfnet import NFNet\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "\n",
        "def set_seed(seed = 0):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjovbIcmdgO9",
        "outputId": "78ef5531-d8f9-4e9c-b3ac-40703e501e9c"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.32.102.114:8470\n",
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUGz_5r4dgLh"
      },
      "source": [
        "EPOCHS = 15\n",
        "FOLDS = 5\n",
        "FOLD_TO_TRAIN = 0\n",
        "OLD_DIM = 256\n",
        "DIM = 512\n",
        "SEED = 42\n",
        "IMAGE_SIZE = [DIM, DIM]\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "GCS_DS_PATH = \"gs://kds-03e1a10468eed52cf1f7962028d75ca9e9ca67e4b3c30a4f640075f4\"\n",
        "LR = 1e-4\n",
        "BATCH_SIZE = strategy.num_replicas_in_sync*16\n",
        "PRETRAINED_MODEL = 'efficientnetv2-xl-21k'\n",
        "STEPS_PER_EPOCH = 2508*(FOLDS-1)/BATCH_SIZE\n",
        "TTA_STEPS = 5\n",
        "NFNET_VARIANT = 'F1'\n",
        "hub_url = f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/{PRETRAINED_MODEL}/feature-vector'\n",
        "MODEL_GCS_PATH = 'gs://kds-fe3e67555fd7fb37eb13b8ccd52726e27768b43c6a31332c1a38a9e5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpKW7aD2dgJA"
      },
      "source": [
        "sample_sub = pd.read_csv('/content/drive/MyDrive/Kaggle/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEc-WyFsdgGq"
      },
      "source": [
        "sample_sub['path'] = sample_sub['id'].progress_apply(lambda x:  f\"{GCS_DS_PATH}/test/{x[0]}/{x}.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBOGxfo4dgEN"
      },
      "source": [
        "def build_decoder(with_labels=True, target_size=(256, 256), ext='npy'):\n",
        "    def decode(path):\n",
        "        file_bytes = tf.io.read_file(path)\n",
        "        if ext == 'npy':\n",
        "            img = tf.io.decode_raw(file_bytes, tf.float16)\n",
        "            img = img[64:]\n",
        "            img = tf.reshape(img, [6, 273, 256])\n",
        "            img = tf.concat([img[0,:,:], img[2,:,:], img[4,:,:]], axis=0)\n",
        "            img = tf.stack([img, img, img], axis=-1)\n",
        "            img = tf.cast(img, tf.float32) \n",
        "            img = tf.image.resize(img, target_size)\n",
        "        else:\n",
        "            if ext == 'png':\n",
        "                img = tf.image.decode_png(file_bytes, channels=3)\n",
        "            elif ext in ['jpg', 'jpeg']:\n",
        "                img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
        "            else:\n",
        "                raise ValueError(\"Image extension not supported\")\n",
        "            img = tf.cast(img, tf.float32) / 255.0\n",
        "            img = tf.image.resize(img, target_size)\n",
        "\n",
        "        return img\n",
        "    \n",
        "    def decode_with_labels(path, label):\n",
        "        return decode(path), label\n",
        "    \n",
        "    return decode_with_labels if with_labels else decode\n",
        "\n",
        "\n",
        "def build_augmenter(with_labels=True):\n",
        "    def augment(img):\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_flip_up_down(img)\n",
        "        return img\n",
        "    \n",
        "    def augment_with_labels(img, label):\n",
        "        return augment(img), label\n",
        "    \n",
        "    return augment_with_labels if with_labels else augment\n",
        "\n",
        "\n",
        "def build_dataset(paths, labels=None, bsize=128, cache=True,\n",
        "                  decode_fn=None, augment_fn=None,\n",
        "                  augment=True, repeat=True, shuffle=1024, \n",
        "                  cache_dir=\"\"):\n",
        "    if cache_dir != \"\" and cache is True:\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "    \n",
        "    if decode_fn is None:\n",
        "        decode_fn = build_decoder(labels is not None)\n",
        "    \n",
        "    if augment_fn is None:\n",
        "        augment_fn = build_augmenter(labels is not None)\n",
        "    \n",
        "    AUTO = tf.data.experimental.AUTOTUNE\n",
        "    slices = paths if labels is None else (paths, labels)\n",
        "    \n",
        "    dset = tf.data.Dataset.from_tensor_slices(slices)\n",
        "    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n",
        "    dset = dset.cache(cache_dir) if cache else dset\n",
        "    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n",
        "    dset = dset.repeat() if repeat else dset\n",
        "    dset = dset.shuffle(shuffle) if shuffle else dset\n",
        "    dset = dset.batch(bsize).prefetch(AUTO)\n",
        "    \n",
        "    return dset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITE5kDmGfkn3"
      },
      "source": [
        "def transform(image,label):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rot = 1.* tf.random.normal([1], dtype='float32', seed=42)\n",
        "    shr = tf.random.normal([1], dtype='float32', seed=42) \n",
        "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32', seed=42)/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32', seed=42)/10.\n",
        "    h_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "    w_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "  \n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = tf.keras.backend.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = tf.keras.backend.cast(idx2,dtype='int32')\n",
        "    idx2 = tf.keras.backend.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
        "        \n",
        "    return tf.reshape(d,[DIM,DIM,3]),label\n",
        "\n",
        "def transform_test(image):\n",
        "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
        "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    XDIM = DIM%2 #fix for size 331\n",
        "    \n",
        "    rot = 1.* tf.random.normal([1], dtype='float32', seed=42)\n",
        "    shr = tf.random.normal([1], dtype='float32', seed=42) \n",
        "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32', seed=42)/10.\n",
        "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32', seed=42)/10.\n",
        "    h_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "    w_shift = 16. * tf.random.normal([1], dtype='float32', seed=42) \n",
        "  \n",
        "    # GET TRANSFORMATION MATRIX\n",
        "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
        "\n",
        "    # LIST DESTINATION PIXEL INDICES\n",
        "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
        "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
        "    z = tf.ones([DIM*DIM],dtype='int32')\n",
        "    idx = tf.stack( [x,y,z] )\n",
        "    \n",
        "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
        "    idx2 = tf.keras.backend.dot(m,tf.cast(idx,dtype='float32'))\n",
        "    idx2 = tf.keras.backend.cast(idx2,dtype='int32')\n",
        "    idx2 = tf.keras.backend.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
        "    \n",
        "    # FIND ORIGIN PIXEL VALUES           \n",
        "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
        "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
        "        \n",
        "    return tf.reshape(d,[DIM,DIM,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvkt97xZftcu"
      },
      "source": [
        "AUG_BATCH = BATCH_SIZE\n",
        "\n",
        "def cutmix(image, label, PROBABILITY = 1.0):\n",
        "    \n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with cutmix applied\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    CLASSES = 1\n",
        "    label = tf.cast(label, 'float32')\n",
        "    \n",
        "    imgs = []; labs = []\n",
        "    for j in range(AUG_BATCH):\n",
        "        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n",
        "        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n",
        "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
        "        # CHOOSE RANDOM LOCATION\n",
        "        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
        "        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n",
        "        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n",
        "        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\n",
        "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
        "        yb = tf.math.minimum(DIM,y+WIDTH//2)\n",
        "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
        "        xb = tf.math.minimum(DIM,x+WIDTH//2)\n",
        "        # MAKE CUTMIX IMAGE\n",
        "        one = image[j,ya:yb,0:xa,:]\n",
        "        two = image[k,ya:yb,xa:xb,:]\n",
        "        three = image[j,ya:yb,xb:DIM,:]\n",
        "        middle = tf.concat([one,two,three],axis=1)\n",
        "        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n",
        "        imgs.append(img)\n",
        "        # MAKE CUTMIX LABEL\n",
        "        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n",
        "\n",
        "        lab1 = label[j,]\n",
        "        lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "            \n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH, DIM,DIM,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH, 1))\n",
        "    return image2,label2\n",
        "\n",
        "def mixup(image, label, PROBABILITY = 1.0):\n",
        "    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n",
        "    # output - a batch of images with mixup applied\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    CLASSES = 1\n",
        "    label = tf.cast(label, 'float32')\n",
        "    \n",
        "    imgs = []; labs = []\n",
        "    for j in range(AUG_BATCH):\n",
        "        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n",
        "        # CHOOSE RANDOM\n",
        "        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
        "        a = 0.5 #tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\n",
        "        # MAKE MIXUP IMAGE\n",
        "        img1 = image[j,]\n",
        "        img2 = image[k,]\n",
        "        imgs.append((1-a)*img1 + a*img2)\n",
        "        # MAKE CUTMIX LABEL\n",
        "\n",
        "        lab1 = label[j,]\n",
        "        lab2 = label[k,]\n",
        "        labs.append((1-a)*lab1 + a*lab2)\n",
        "\n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
        "    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,1))\n",
        "    return image2,label2\n",
        "\n",
        "def transform_cutmix_mixup(image,label):\n",
        "    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n",
        "    DIM = IMAGE_SIZE[0]\n",
        "    SWITCH = 0.5\n",
        "    CUTMIX_PROB = 0.666\n",
        "    MIXUP_PROB = 0.666\n",
        "    label = tf.cast(label, 'float32')\n",
        "    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n",
        "    image2, label2 = cutmix(image, label, CUTMIX_PROB)\n",
        "    image3, label3 = mixup(image, label, MIXUP_PROB)\n",
        "    imgs = []; labs = []\n",
        "    for j in range(AUG_BATCH):\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n",
        "        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n",
        "        labs.append(P*label2[j,]+(1-P)*label3[j,])\n",
        "    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n",
        "    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
        "    label4 = tf.reshape(tf.stack(labs),(AUG_BATCH,1))\n",
        "    return image4,tf.reshape(tf.cast(tf.math.round(label4), 'int64'), (-1,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0wJk-QMftTf"
      },
      "source": [
        "LR_START = LR\n",
        "LR_MAX = LR*4\n",
        "LR_MIN = LR/10\n",
        "LR_RAMPUP_EPOCHS = 5\n",
        "LR_SUSTAIN_EPOCHS = 0\n",
        "LR_EXP_DECAY = .8\n",
        "\n",
        "def CustomSchedule(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
        "    return lr\n",
        "\n",
        "def ExponentialSchedule(epoch, lr):\n",
        "    rate = DECAY_RATE\n",
        "    return lr*tf.math.exp(-rate*(epoch)).numpy()\n",
        "\n",
        "\n",
        "CustomCallback = tf.keras.callbacks.LearningRateScheduler(CustomSchedule, verbose = True)\n",
        "EarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_AUC', patience=10, mode='max', restore_best_weights=True)\n",
        "ReduceOnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_AUC', mode='max', min_lr=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKC6fwspf0_j"
      },
      "source": [
        "paths = sample_sub['path'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnY7tD_f0kf"
      },
      "source": [
        "with strategy.scope():\n",
        "    \n",
        "    ############# B6 #############\n",
        "    \n",
        "    # model1 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B6/B6_mixup_0_V2.h5')\n",
        "    # model2 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B6/B6_mixup_1_V2.h5')\n",
        "    # model3 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B6/B6_mixup_2_V2.h5')\n",
        "    # model4 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B6/B6_mixup_3_V2.h5')\n",
        "    # model5 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B6/B6_mixup_4_V2.h5')\n",
        "    \n",
        "    ############# B7 #############\n",
        "    \n",
        "    # model6 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B7/B7_mixup_0_V2.h5')\n",
        "    # model7 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B7/B7_mixup_1_V2.h5')\n",
        "    # model8 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B7/B7_mixup_2_V2.h5')\n",
        "    # model9 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B7/B7_mixup_3_V2.h5')\n",
        "    # model10 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/EfficientNet-B7/B7_mixup_4_V2.h5')\n",
        "    \n",
        "#     ############# V2-m-21k #############\n",
        "    \n",
        "#     model11 = tf.keras.models.load_model('../input/efficientnetv2m21k/V2-m/efficientnetv2-m-21k_finetuned_0.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "#     model12 = tf.keras.models.load_model('../input/efficientnetv2m21k/V2-m/efficientnetv2-m-21k_finetuned_1.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "#     model13 = tf.keras.models.load_model('../input/efficientnetv2m21k/V2-m/efficientnetv2-m-21k_finetuned_2.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "#     model14 = tf.keras.models.load_model('../input/efficientnetv2m21k/V2-m/efficientnetv2-m-21k_finetuned_3.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "#     model15 = tf.keras.models.load_model('../input/efficientnetv2m21k/V2-m/efficientnetv2-m-21k_finetuned_4.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "    \n",
        "    ############# V2-xl-21k #############\n",
        "    \n",
        "    model16 = tf.keras.models.load_model('../input/efficientnetv2xl21k/V2-xl/efficientnetv2-xl-21k_finetuned_0.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "    model17 = tf.keras.models.load_model('../input/efficientnetv2xl21k/V2-xl/efficientnetv2-xl-21k_finetuned_1.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "    model18 = tf.keras.models.load_model('../input/efficientnetv2xl21k/V2-xl/efficientnetv2-xl-21k_finetuned_2.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "    model19 = tf.keras.models.load_model('../input/efficientnetv2xl21k/V2-xl/efficientnetv2-xl-21k_finetuned_3.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "    model20 = tf.keras.models.load_model('../input/efficientnetv2xl21k/V2-xl/efficientnetv2-xl-21k_finetuned_4.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "\n",
        "    # save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "\n",
        "#     ############# F0 #############\n",
        "        \n",
        "#     model21 = tf.keras.models.load_model('../input/nfnet-finetuned/NFNet_finetuned/F0/0_finetuned', options=save_locally)\n",
        "#     model22 = tf.keras.models.load_model('../input/nfnet-finetuned/NFNet_finetuned/F0/1_finetuned', options=save_locally)\n",
        "#     model23 = tf.keras.models.load_model('../input/nfnet-finetuned/NFNet_finetuned/F0/2_finetuned', options=save_locally)\n",
        "#     model24 = tf.keras.models.load_model('../input/nfnet-finetuned/NFNet_finetuned/F0/3_finetuned', options=save_locally)\n",
        "#     model25 = tf.keras.models.load_model('../input/nfnet-finetuned/NFNet_finetuned/F0/4_finetuned', options=save_locally)\n",
        "\n",
        "    ############# F1 #############\n",
        "       \n",
        "    # model26 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F1/0_finetuned_V2', options=save_locally)\n",
        "    # model27 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F1/1_finetuned_V2', options=save_locally)\n",
        "    # model28 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F1/2_finetuned_V2', options=save_locally)\n",
        "    # model29 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F1/3_finetuned_V2', options=save_locally)\n",
        "    # model30 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F1/4_finetuned_V2', options=save_locally)\n",
        "\n",
        "    ############# F2 #############\n",
        "       \n",
        "    # model31 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F2/0_finetuned_V2', options=save_locally)\n",
        "    # model32 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F2/1_finetuned_V2', options=save_locally)\n",
        "    # model33 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F2/2_finetuned_V2', options=save_locally)\n",
        "    # model34 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F2/3_finetuned_V2', options=save_locally)\n",
        "    # model35 = tf.keras.models.load_model('/content/drive/MyDrive/Kaggle/weights/Finetuned_weights/NFNet-F2/4_finetuned_V2', options=save_locally)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW-1R3gDncY5"
      },
      "source": [
        "decoder = build_decoder(with_labels=False, target_size=IMAGE_SIZE, ext='npy')\n",
        "test_dataset = build_dataset(\n",
        "    paths,\n",
        "    bsize=BATCH_SIZE, \n",
        "    decode_fn=decoder,\n",
        "    shuffle=False, \n",
        "    repeat=False, \n",
        "    augment=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCvL87S3dgBn"
      },
      "source": [
        "predictions = []\n",
        "\n",
        "for i in range(TTA_STEPS):\n",
        "    print(f'Step: {i+1}')\n",
        "    preds = np.mean([\n",
        "        # model1.predict(test_dataset, verbose=1),\n",
        "        # model2.predict(test_dataset, verbose=1),\n",
        "        # model3.predict(test_dataset, verbose=1),\n",
        "        # model4.predict(test_dataset, verbose=1),\n",
        "        # model5.predict(test_dataset, verbose=1), \n",
        "        \n",
        "        # model6.predict(test_dataset, verbose=1),\n",
        "        # model7.predict(test_dataset, verbose=1),\n",
        "        # model8.predict(test_dataset, verbose=1),\n",
        "        # model9.predict(test_dataset, verbose=1),\n",
        "        # model10.predict(test_dataset, verbose=1),\n",
        "        \n",
        "#         model11.predict(test_dataset, verbose=1),\n",
        "#         model12.predict(test_dataset, verbose=1),\n",
        "#         model13.predict(test_dataset, verbose=1),\n",
        "#         model14.predict(test_dataset, verbose=1),\n",
        "#         model15.predict(test_dataset, verbose=1),\n",
        "        \n",
        "#         model16.predict(test_dataset, verbose=1),\n",
        "#         model17.predict(test_dataset, verbose=1),\n",
        "#         model18.predict(test_dataset, verbose=1),\n",
        "#         model19.predict(test_dataset, verbose=1),\n",
        "#         model20.predict(test_dataset, verbose=1),\n",
        "        \n",
        "#         model21.predict(test_dataset, verbose=1),\n",
        "#         model22.predict(test_dataset, verbose=1),\n",
        "#         model23.predict(test_dataset, verbose=1),\n",
        "#         model24.predict(test_dataset, verbose=1),\n",
        "#         model25.predict(test_dataset, verbose=1),\n",
        "        \n",
        "        model26.predict(test_dataset, verbose=1),\n",
        "        model27.predict(test_dataset, verbose=1),\n",
        "        model28.predict(test_dataset, verbose=1),\n",
        "        model29.predict(test_dataset, verbose=1),\n",
        "        model30.predict(test_dataset, verbose=1),\n",
        "\n",
        "        model31.predict(test_dataset, verbose=1),\n",
        "        model32.predict(test_dataset, verbose=1),\n",
        "        model33.predict(test_dataset, verbose=1),\n",
        "        model34.predict(test_dataset, verbose=1),\n",
        "        model35.predict(test_dataset, verbose=1)\n",
        "        \n",
        "    ], axis=0)\n",
        "    predictions.append(preds)\n",
        "    \n",
        "final_predictions = np.mean(predictions, axis=0).reshape(-1,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHJd_YB2gY6v"
      },
      "source": [
        "sample_sub['target'] = final_predictions\n",
        "sample_sub = sample_sub.drop(columns='path')\n",
        "sample_sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewPOtkPXgfXv"
      },
      "source": [
        "sample_sub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFWpNoaMnfj8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}